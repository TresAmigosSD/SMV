#!/usr/bin/env bash

set -e

THIS_FILE_DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" >/dev/null && pwd )"
source "${THIS_FILE_DIR}/_env.sh"
SMV_TOOLS="$(get_smv_tools_dir)"

export SPARK_PRINT_LAUNCH_COMMAND=1

# Set PYTHONSTARTUP to load the init script
# Since pyspark does not take app parameters, we have to pass SMV_ARGS to the
# startup script. A little hackish before we figure out better ways.
echo "${SMV_ARGS[@]}" > .smv_shell_all_args
SMV_ARGS=()

OLD_PYTHONSTARTUP=$PYTHONSTARTUP
export PYTHONSTARTUP="${SMV_TOOLS}/../src/main/python/scripts/smvpyshell.py"


# PySpark pre-2.0.0 has a bug (see
# https://issues.apache.org/jira/browse/SPARK-5185) that does not add
# the jar file to the driver's classpath, so we need to add the jars
# to the --driver-class-path command-line option
SMV_LAUNCH_SCRIPT="${SMV_PYSPARK_FULLPATH}" run_pyspark_with

# Reset PYTHONSTARTUP
export PYTHONSTARTUP=$OLD_PYTHONSTARTUP

rm -f .smv_shell_all_args
